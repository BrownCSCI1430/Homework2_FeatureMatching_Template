%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CSCI 1430 Written Question Template
%
% This is a LaTeX document. LaTeX is a markup language for producing documents. 
% You will fill out this document, compile it into a PDF document, then upload the PDF to Gradescope. 
%
% To compile it into a PDF you can:
% - Use vscode! Install a LaTeX distribution (all common OS): http://www.latex-project.org/get/ 
% + VSCode extension: https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop
% - Use an online tool: https://www.overleaf.com/ - most LaTeX packages are pre-installed here (e.g., \usepackage{}).
%
% If you need help with LaTeX, please come to office hours.
% Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% The CSCI1430 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{csci1430}

\begin{document}
\title{Homework 2 Written Questions}
\HomeworkShortName{HW2}
\maketitle

\writeinstructions

% Please leave the pagebreak
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=10] 
Let's look again at the Fourier decomposition demo that we saw in class. The Fourier transform lets us operate on images with respect to their frequency content via the \textbf{amplitude} (or magnitude) and the \textbf{phase}. Navigate to the \texttt{questions} folder. Then, within your virtual environment, run the following command:
\begin{verbatim}
$ uv pip install dearpygui
$ cd questions/fourierdemo
$ uv run python liveFourier.py
\end{verbatim}
Play with the demo!
To consider the properties of the Fourier domain, it will help to use 'Cat Mode' so that we see a fixed image (of a cat).

Then, for each transform listed in the rows below, please place \filled~in the columns that best describe the effects. There may be multiple for each row, or none.

\begin{itemize}
    \item In the demo, you may see minor changes in amplitude/phase due to image resampling, quantization, and clipping; try to ignore these and think about the \textbf{fundamental properties}: what should happen to amplitude and phase?
    \item $I$ represents the image intensity, $x$ represents the image's pixel coordinates.
    \item ``Scales same'' means shrinks when the input shrinks. \item ``Scales inverse'' means expands when the input shrinks.
\end{itemize}
\end{question}

\begin{answer}\noindent
\setlength{\tabcolsep}{11pt}
\begin{tabular}{l|c|c|c|c|c|c|c}
\textbf{Input transform} & \rotatebox{90}{\textbf{Invariant}} & \rotatebox{90}{\textbf{Brighter}} & \rotatebox{90}{\textbf{DC only}} & \rotatebox{90}{\textbf{2D translates}} & \rotatebox{90}{\textbf{2D rotates}} & \rotatebox{90}{\textbf{2D scales same}} & \rotatebox{90}{\textbf{2D scales inverse}} \\
\hline
\textbf{Change intensity} ($I += 20$) & & & & & & & \\
\hspace{0.1cm} \textit{Effect on amplitude} & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hspace{0.1cm} \textit{Effect on phase}     & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hline
\textbf{Change intensity} ($I *= 1.2$) & & & & & & & \\
\hspace{0.1cm} \textit{Effect on amplitude} & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hspace{0.1cm} \textit{Effect on phase}     & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hline
\textbf{2D translate} ($x += 20$) & & & & & & & \\
\hspace{0.1cm} \textit{Effect on amplitude} & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hspace{0.1cm} \textit{Effect on phase}     & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hline
\textbf{2D rotate} ($x$ by $45^{\circ}$) & & & & & & & \\
\hspace{0.1cm} \textit{Effect on amplitude} & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hspace{0.1cm} \textit{Effect on phase}     & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hline
\textbf{2D scale} ($x *= 0.5$) & & & & & & & \\
\hspace{0.1cm} \textit{Effect on amplitude} & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\hspace{0.1cm} \textit{Effect on phase}     & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox & \cbox \\
\end{tabular}
\end{answer}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=4]
In class, we saw another demo that connected the Fourier domain to convolution. 
\begin{verbatim}
$ uv run python liveConvolutionTheorem.py
\end{verbatim}
Play with the demo!

In your own words, what is the connection between the Fourier domain and convolution, and what benefits can it provide? \textbf{[3-5 sentences]}
\end{question}

\begin{answer}[height=16]
TODO: Your answer here
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%

Often in computer vision, we wish to find points in one image that match to the same world point in another image---so-called correspondence finding.

\begin{question}[points=9] 
In the following three pairs of images, please use the included python script \texttt{plot\_corners.py} to find corners using the Harris corner detection algorithm. Run the script with \texttt{python plot\_corners.py \{example\}} where \texttt{\{example\}} is one of \{\texttt{RISHLibrary}, \texttt{Chase}, \texttt{LaddObservatory}\}.

For each pair, discuss any differences in the returned corners, the properties of the images that create these differences, and any underlying real world phenomena that may have been the primary cause of these differences. \textbf{[3--5 sentences]}
\end{question}

RISHLibrary: [3 points]
\href{images/RISHLibrary1.jpg}{RISHLibrary1.jpg} and \href{images/RISHLibrary2.jpg}{RISHLibrary2.jpg} 

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_RishLibrary1.png}

TODO: Your answer here
\end{answer}

\pagebreak

Chase: [3 points]
\href{images/Chase1.jpg}{Chase1.jpg} and \href{images/Chase2.jpg}{Chase2.jpg}

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_Chase1.png}

TODO: Your answer here
\end{answer}
    
\pagebreak


LaddObservatory: [3 points]
\href{images/LaddObservatory1.jpg}{LaddObservatory1.jpg} and \href{images/LaddObservatory2.jpg}{LaddObservatory2.jpg}

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_Ladd1.png}

TODO: Your answer here
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=3,drawbox=false]
As we just saw, the Harris Corner Detector can find feature points for image matching.

To detect these features, the algorithm approximates the strength of any change in the auto-correlation function $E(u, v)$ over shifts $u,v$. We look at the second derivative term in the Taylor series expansion of $E$ to consider the local shape of $E(u, v)$ via the \emph{structure tensor} $M$ (often also called the second moment matrix). Then, we analyze $M$ to determine if that pixel is a corner or not.

\textit{Note:} We expect this conceptual question to require careful study of the lecture slides.
\end{question}

\begin{orangebox}
How do the eigenvalues of the $M$ structure tensor vary with local image brightness, and how might we interpret the eigenvalues geometrically as an ellipse? \textbf{[6--8 sentences]}
\end{orangebox}

\begin{answer}[height=16]
TODO: Your answer here.
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=9,drawbox=false] 
One use of local feature description and matching is \href{https://youtu.be/xD88Qs_DZp4?t=12}{fingerprint recognition}---please watch the video and note the similarity to our task in this homework.

\begin{minipage}{0.7\linewidth}
Fingerprint recognition uses biometric data---measurements of human biological features that are unique to an individual---to unlock doors or devices quickly and without needing to remember a password. However, given its uniqueness, biometric data may be seen as a greater privacy encroachment upon a person than a password. Further, given the trust that is derived from the uniqueness of biometric data, it may also pose a greater risk of misuse if the data is not secure because the data cannot be changed.
\end{minipage}\hspace{0.05\linewidth}
\begin{minipage}{0.2\linewidth}
\includegraphics[width=\linewidth]{./images/fingerprint.jpg}
\end{minipage}
\end{question}

\begin{subquestion}[points=3]
Do you use biometric recognition systems? List them. [If not, list some that people around you use.]

For one of the systems you use, where is the reference data stored (such as your stored fingerprint), where does the authentication happen, and how does the authentication happen (at a high level)? 
If you don't know, then try to find the answer online. \textbf{[4--6 sentences]}
\end{subquestion}
    
\begin{answer}[height=14]
TODO your answer here.
\end{answer}

\pagebreak

\begin{subquestion}[points=3]
How might someone use computer vision to steal or spoof your biometric data to gain access? \emph{This could be across reconstruction, recognition, or (re)organization.} \textbf{[3--5 sentences]}
\end{subquestion}

\begin{answer}[height=14]
TODO your answer here.
\end{answer}
    

\begin{subquestion}[points=3]
Biometric recognition systems may not affect all people equally. For a biometric identification system, define a group of people and describe how they might be affected disproportionately. \textbf{[3--5 sentences]}
\end{subquestion}
    
\begin{answer}[height=14]
TODO your answer here.
\end{answer}
    

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{subquestion}[points=7,drawbox=false] 
Brown University decides to entirely replace passwords with biometric data to authenticate student identity on its computer systems. Given how accurate your feature matching homework 2 code is, Brown asks you to develop the authentication system as your CSCI 1430 final project. Lucky you.

In preparation, you read a previous case about a biometric data breach.\newline (See \textbf{BioStar2\_DataBreach.md})
\end{subquestion}

\begin{subquestion}[points=3]
How were BioStar 2 storing their fingerprint data? Knowing the computer vision algorithms involved in feature matching, what different processing, features, or storage might you consider instead to decrease the risk of a biometric data breach? \textbf{[4--6 sentences]}
\end{subquestion}

\begin{answer}[height=16]
TODO: Your answer here.
\end{answer}

\pagebreak

\begin{subquestion}[points=4,drawbox=false]
Even though fingerprints are thought to be unique, we are bound by the accuracy of computer vision systems to detect and recognize that uniqueness.
This may be a challenge for Brown's 10,000 students, let alone a national-scale database such as the FBI's \href{https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi}{Next Generation Identification System} that houses around 190 million prints and conducts 170,000 daily assessments (Jan.~2026; see \textbf{NGI\_Jan2026\_SystemFactSheet.pdf}). NGI's Advanced Fingerprint Identification Technology is claimed to be 99.6\% accurate.

Even seemingly high accuracies can create many inaccurate matches with large databases, potentially causing inaccurate judgements in criminal cases.
\end{subquestion}

\begin{orangebox}
Fingerprint images are quite different from the images of natural `human-scale' scenes we've been using throughout the course. Is correspondence finding for fingerprint images an easier problem than for natural images? Why or why not? Think about the variation within each domain of images, the accuracy required, the consequences of a match (or lack thereof), the accuracy required, and the scope of the application. \textbf{[6--8 sentences]}

\vspace{0.25cm}
\emph{Feel free to refer to the video at the beginning of Q3, and for more detail please refer to \href{http://biometrics.cse.msu.edu/Presentations/AnilJain_UniquenessOfFingerprints_NAS05.pdf}{the first 15 slides of this deck}---it has example fingerprints and additional information.}
\end{orangebox}

\begin{answer}[height=16]
TODO: Your answer here.
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\writefeedback

\end{document}
