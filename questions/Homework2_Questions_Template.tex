%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CSCI 1430 Written Question Template
%
% This is a LaTeX document. LaTeX is a markup language for producing documents. 
% You will fill out this document, compile it into a PDF document, then upload the PDF to Gradescope. 
%
% To compile into a PDF on department machines:
% > pdflatex thisfile.tex
%
% If you do not have LaTeX, your options are:
% - Personal laptops (all common OS): http://www.latex-project.org/get/ 
% + VSCode extension: https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop
% - Online Tool: https://www.overleaf.com/ - most LaTeX packages are pre-installed here (e.g., \usepackage{}).
%
% If you need help with LaTeX, please come to office hours.
% Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% The CSCI1430 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{csci1430}

\begin{document}
\title{Homework 2 Written Questions}
\maketitle
\thispagestyle{fancy}

\writeinstructions

\section*{This Homework}
\begin{itemize}
    \item 6 questions \textbf{[10 + 9 + 9 + 7 + 3 + 5 = 43 points]}.
    \item Include code, images, and equations where appropriate.
\end{itemize}

% Please leave the pagebreak
\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{question}[points=10] 
Let's look again at the webcam Fourier decomposition demo that we saw in class. The Fourier transform lets us operate on images with respect to their frequency content. Navigate to the \texttt{questions} folder. Then, within your virtual environment, run the following command:
\begin{verbatim}
$ python liveFFT2.py
\end{verbatim}
\end{question}

\emph{Note: Your computer must have a webcam. If it does not, find the \textbf{use\_webcam} variable and set it to false. If you're running an exotic setup, like from within WSL, then your webcam might not be accessible.}
\\
\\
The python file contains five parts for you to explore the effect of manipulating the Fourier decomposition representation upon the reconstructed image. In the demo, top left is the input image, bottom left is the basis frequency amplitude image, bottom right is the corresponding phase image, and top right is the reconstruction of the input image via the Fourier representation.

\begin{itemize}
    \item Part 0: Scanning the basis and observing the output image.
    \item Part 1: Reconstructions from different numbers of basis frequencies.
    \item Part 2: Replacing amplitude and phase with that from a different image.
    \item Part 3: Replacing amplitude and phase with that from a noise image.
    \item Part 4: Manipulating the amplitude and phase images.
\end{itemize}

Uncomment the different parts and explore!
\\
\\
You \textbf{will} have to answer short responses and screenshots in the following parts so you might want to answer them as you go!
\\
\\
\emph{Note:} Please keep grading anonymous. Use your cat as a model, show your favourite vector calculus book, etc. Extra credit for creative effort.

\newpage

In the following subquestions, please include a screenshot for the given part as well as answer a short response. \textbf{[1-2 sentences]}.

\begin{subquestion}[points=2]
In \textbf{Part 0}, the reconstructed image (top right) looks like bars of black and white lines rather than the image to the left of it. Why is this? Hint: Look what we do after zero-ing out the amplitude and phase.
\end{subquestion}

\begin{answer}[height=15]
\includegraphics[width=0.4\textwidth,keepaspectratio]{images/TODO_Part0.png}

TODO: Your answer for (a) here
\end{answer}

\begin{subquestion}[points=2]
In \textbf{Part 1}, what is the relation between how many bases are applied and the reconstructed image (top right image)?
\end{subquestion}

\begin{answer}[height=15]
\includegraphics[width=0.4\textwidth,keepaspectratio]{images/TODO_Part1.png}

TODO: Your answer here
\end{answer}

\newpage

\begin{subquestion}[points=2]
In \textbf{Part 2} and \textbf{Part 3}, after replacing the phase/amplitude with the phase/amplitude of the image of Merlin the cat or the noisy image, which do you think contributes more to the reconstruction?
\end{subquestion}

\begin{answer}[height=16]
\includegraphics[width=0.4\textwidth,keepaspectratio]{images/TODO_Part2.png}
\includegraphics[width=0.4\textwidth,keepaspectratio]{images/TODO_Part3.png}
        
TODO: Your answer here
\end{answer}

\begin{subquestion}[points=2]
After \textbf{Part 4}, describe the effect of one modification you saw in the demo after playing around with the phase or amplitude in relation to the reconstructed image.
\end{subquestion}

\begin{answer}[height=16]
\includegraphics[width=0.4\textwidth,keepaspectratio]{images/TODO_Part4.png}
    
TODO: Your answer here.
\end{answer}

\newpage

\begin{subquestion}[points=2]
In the demo you saw how the Fourier Transform deconstructs an image into amplitude and phase, then subsequently transforms it back into a reconstructed image. This process has a use in relation to convolution that we discussed in lecture. In your own words, what is The Convolution Theorem and what benefits can it provide? \textbf{[2-3 sentences]}
\end{subquestion}

\begin{answer}[height=16]
TODO: Your answer here
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%


Often in computer vision, we wish to find points in one image that match to the same world point in another image---so-called correspondence finding.

\begin{question}[points=9] 
In the following three pairs of images, please use the included python script \texttt{plot\_corners.py} to find corners using the Harris corner detection algorithm. Run the script with \texttt{python plot\_corners.py \{example\}} where \texttt{\{example\}} is one of \{\texttt{RISHLibrary}, \texttt{Chase}, \texttt{LaddObservatory}\}.

For each pair, discuss any differences in the returned corners, the properties of the images that create these differences, and any underlying real world phenomena that may have been the primary cause of these differences. \textbf{[3--5 sentences]}
\end{question}

RISHLibrary: [3 points]
\href{images/RISHLibrary1.jpg}{RISHLibrary1.jpg} and \href{images/RISHLibrary2.jpg}{RISHLibrary2.jpg} 

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_RishLibrary1.png}

TODO: Your answer here
\end{answer}

\pagebreak

Chase: [3 points]
\href{images/Chase1.jpg}{Chase1.jpg} and \href{images/Chase2.jpg}{Chase2.jpg}

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_Chase1.png}

TODO: Your answer here
\end{answer}
    
\pagebreak


LaddObservatory: [3 points]
\href{images/LaddObservatory1.jpg}{LaddObservatory1.jpg} and \href{images/LaddObservatory2.jpg}{LaddObservatory2.jpg}

\begin{answer}[height=22]
\includegraphics[width=0.5\textwidth,height=7cm,keepaspectratio]{images/TODO_Ladd1.png}

TODO: Your answer here
\end{answer}


\pagebreak

\begin{question}[points=9,drawbox=false] 
One use of local feature description and matching is \href{https://youtu.be/xD88Qs_DZp4?t=11}{fingerprint recognition}---please watch the video and note the similarity to our task in this homework. 

Fingerprint recognition uses biometric data---measurements of human biological features that are unique to an individual---to make it convenient to unlock doors or devices quickly and without needing to remember a password. However, given its uniqueness, biometric data may be seen as a greater privacy encroachment upon a person than a password. Further, given the trust that is derived from the uniqueness of biometric data, it may also pose a greater risk of misuse if the data is not secure because the data cannot be changed.
\end{question}

\begin{subquestion}[points=3]
Do you use biometric recognition systems? List them. [If not, list some that people around you use.]
For one of the systems you use, where is the reference data stored (such as your stored fingerprint), where does the authentication happen, and how does the authentication happen (at a high level)? 
If you don't know, then try to find the answer online. \textbf{[4--6 sentences]}
\end{subquestion}
    
\begin{answer}[height=14]
TODO your answer here.
\end{answer}

\pagebreak

\begin{subquestion}[points=3]
How might someone use computer vision to steal or spoof your biometric data to gain access? \emph{This could be across reconstruction, recognition, or (re)organization.} \textbf{[3--5 sentences]}
\end{subquestion}

\begin{answer}[height=14]
TODO your answer here.
\end{answer}
    

\begin{subquestion}[points=3]
Biometric recognition systems may not affect all people equally. For a biometric identification system, define a group of people and describe how they might be affected disproportionately. \textbf{[3--5 sentences]}
\end{subquestion}
    
\begin{answer}[height=14]
TODO your answer here.
\end{answer}
    

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{question}[points=7,drawbox=false] 
Brown University decides to entirely replace passwords with biometric data to authenticate student identity on its computer systems. Given how accurate your feature matching homework 2 code is, Brown asks you to develop the authentication system as your CSCI 1430 final project. Lucky you.

In preparation, you read a previous case about a \href{https://www.vpnmentor.com/blog/report-biostar2-leak/}{biometric data breach}.
\end{question}

\begin{subquestion}[points=3]
How were BioStar 2 storing their fingerprint data? Knowing the computer vision algorithms involved in feature matching, what different processing, features, or storage might you consider instead to decrease the risk of a biometric data breach? \textbf{[4--6 sentences]}
\end{subquestion}

\begin{answer}[height=16]
TODO: Your answer here.
\end{answer}

\pagebreak

\begin{subquestion}[points=4,drawbox=false]
Even though fingerprints are thought to be unique, we are bound by the accuracy of computer vision systems to detect and recognize that uniqueness.
This may be a challenge for Brown's 10,000 students, let alone a national-scale database such as the FBI's \href{https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi}{Next Generation Identification System} that houses over \href{https://en.wikipedia.org/wiki/Next_Generation_Identification}{100 million fingerprints (since 2014)}; its Advanced Fingerprint Identification Technology is claimed to be 99.6\% accurate.

Even seemingly high accuracies can create many inaccurate matches with large databases, potentially causing inaccurate judgements in criminal cases.
\end{subquestion}

\begin{orangebox}
Fingerprint images are quite different from the images of natural `human-scale' scenes we've been using throughout the course. Is correspondence finding for fingerprint images an easier problem than for natural images? Why or why not? Think about the variation within each domain of images, the accuracy required, the consequences of a match (or lack thereof), the accuracy required, and the scope of the application. \textbf{[6--8 sentences]}

\emph{Refer to the video at the beginning of Q3, and for more detail please refer to \href{http://biometrics.cse.msu.edu/Presentations/AnilJain_UniquenessOfFingerprints_NAS05.pdf}{the first 15 slides of this deck}---it has example fingerprints and additional information.}
\end{orangebox}

\begin{answer}[height=16]
TODO: Your answer here.
\end{answer}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{question}[points=3,drawbox=false]
As we saw in Q2, the Harris Corner Detector can find feature points for image matching.

To detect these features, the algorithm approximates the strength of any change in the auto-correlation function $E(u, v)$ over shifts $u,v$. We look at the second derivative term in the Taylor series expansion of $E$ to consider the local shape of $E(u, v)$ via the \emph{structure tensor} $M$. Then, we analyze $M$ to determine if that pixel is a corner or not.

\textit{Note:} We expect this conceptual question to require careful study of the lecture slides.
\end{question}

\begin{orangebox}
How do the eigenvalues of the $M$ structure tensor vary with local image brightness, and how might we interpret the eigenvalues geometrically as an ellipse)? \textbf{6--8 sentences}
\end{orangebox}

\begin{answer}[height=16]
TODO here
\end{answer}

% Please leave the pagebreak
\pagebreak

\begin{question}[points=5,drawbox=false]
The SIFT algorithm can extract feature descriptors around feature points. Given a feature point location, the SIFT algorithm converts a 16$\times$16 window around the feature point into a 128$\times$1 feature descriptor via a radial histogram of the gradient magnitudes within the window.
\end{question}

\begin{orangebox}
Write pseudocode \emph{but with correct matrix/array indices} for descriptor extraction.

\emph{Notes:} Do this for just one interest point at one scale; ignore the overall interest point orientation; ignore the Gaussian weighting; ignore all normalization post-processing; ignore image boundaries; ignore sub-pixel interpolation and just pick an arbitrary center within the 16$\times$16 for your feature descriptor.
\end{orangebox}

\begin{answer}[height=30]
\begin{python}
# You can assume access to the image, x and y gradients, 
# and their magnitudes/orientations.
image = imread("rara.jpg")
grad_x = filter(image, "sobelX")
grad_y = filter(image, "sobelY")
grad_mag = sqrt(grad_x .^2 + grad_y.^2)
grad_ori = atan2(grad_y, grad_x)

# Takes in a interest point x,y location and returns 
# a feature descriptor
def SIFTdescriptor(x, y)
    descriptor = zeros(128,1)
    
    # TODO: Populate descriptor with the right gradient 
    # magnitudes dependent on the gradient orientations
    





    return descriptor
\end{python}
\end{answer}



\pagebreak

\section*{Feedback? (Optional)}
We appreciate your feedback on how to improve the course. You can provide anonymous feedback through \href{https://forms.gle/Eu5jJbDUmLknAyJV9}{this form} which can be accessed using your Brown account (your identity will not be collected). If you have urgent non-anonymous comments/questions, please email the instructor.

\end{document}
